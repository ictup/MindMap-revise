{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "成功读取CSV文件，共有29行\n",
            "列名: ['Question', 'Label', 'MindMap', 'GPT3.5', 'BM25_retrieval', 'Embedding_retrieval', 'KG_retrieval', 'KG_self-consistency', 'KG_TreeOfThoughts', 'GPT4']\n",
            "正在处理第 1/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\python\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:402: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.6351, R: 0.8641, F1: 0.7321\n",
            "  Output2 - P: 0.8464, R: 0.8252, F1: 0.8356\n",
            "  Output3 - P: 0.7548, R: 0.8423, F1: 0.7961\n",
            "  Output4 - P: 0.7125, R: 0.8395, F1: 0.7708\n",
            "  Output5 - P: 0.7009, R: 0.8173, F1: 0.7546\n",
            "  Output6 - P: 0.8249, R: 0.8590, F1: 0.8416\n",
            "  Output7 - P: 0.6582, R: 0.7995, F1: 0.7220\n",
            "  Output8 - P: 0.7787, R: 0.8053, F1: 0.7918\n",
            "平均值 - P: 0.7390, R: 0.8315, F1: 0.7806\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7770, R: 0.9210, F1: 0.8429\n",
            "  Output2 - P: 0.8979, R: 0.8895, F1: 0.8937\n",
            "  Output3 - P: 0.8455, R: 0.8983, F1: 0.8711\n",
            "  Output4 - P: 0.8348, R: 0.8902, F1: 0.8616\n",
            "  Output5 - P: 0.8224, R: 0.8856, F1: 0.8528\n",
            "  Output6 - P: 0.8839, R: 0.9111, F1: 0.8973\n",
            "  Output7 - P: 0.8002, R: 0.8780, F1: 0.8373\n",
            "  Output8 - P: 0.8704, R: 0.8771, F1: 0.8737\n",
            "平均值 - P: 0.8415, R: 0.8939, F1: 0.8663\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7504, R: 0.9041, F1: 0.8201\n",
            "  Output2 - P: 0.8982, R: 0.8736, F1: 0.8857\n",
            "  Output3 - P: 0.8329, R: 0.8870, F1: 0.8591\n",
            "  Output4 - P: 0.8207, R: 0.8768, F1: 0.8479\n",
            "  Output5 - P: 0.8079, R: 0.8615, F1: 0.8338\n",
            "  Output6 - P: 0.8585, R: 0.8967, F1: 0.8772\n",
            "  Output7 - P: 0.7770, R: 0.8511, F1: 0.8124\n",
            "  Output8 - P: 0.8675, R: 0.8617, F1: 0.8645\n",
            "平均值 - P: 0.8266, R: 0.8766, F1: 0.8501\n",
            "--------------------------------------------------\n",
            "正在处理第 2/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6710, R: 0.8355, F1: 0.7443\n",
            "  Output2 - P: 0.7827, R: 0.8555, F1: 0.8175\n",
            "  Output3 - P: 0.7066, R: 0.8188, F1: 0.7585\n",
            "  Output4 - P: 0.7316, R: 0.8340, F1: 0.7795\n",
            "  Output5 - P: 0.7148, R: 0.8276, F1: 0.7670\n",
            "  Output6 - P: 0.8056, R: 0.8684, F1: 0.8358\n",
            "  Output7 - P: 0.6819, R: 0.8297, F1: 0.7486\n",
            "  Output8 - P: 0.7059, R: 0.8329, F1: 0.7641\n",
            "平均值 - P: 0.7250, R: 0.8378, F1: 0.7769\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7838, R: 0.8864, F1: 0.8320\n",
            "  Output2 - P: 0.8655, R: 0.8862, F1: 0.8757\n",
            "  Output3 - P: 0.8253, R: 0.8747, F1: 0.8493\n",
            "  Output4 - P: 0.8411, R: 0.8824, F1: 0.8613\n",
            "  Output5 - P: 0.8387, R: 0.8889, F1: 0.8631\n",
            "  Output6 - P: 0.8686, R: 0.9036, F1: 0.8858\n",
            "  Output7 - P: 0.8096, R: 0.8757, F1: 0.8413\n",
            "  Output8 - P: 0.8312, R: 0.8824, F1: 0.8560\n",
            "平均值 - P: 0.8330, R: 0.8851, F1: 0.8581\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7475, R: 0.8629, F1: 0.8011\n",
            "  Output2 - P: 0.8580, R: 0.8761, F1: 0.8669\n",
            "  Output3 - P: 0.8135, R: 0.8603, F1: 0.8362\n",
            "  Output4 - P: 0.8282, R: 0.8679, F1: 0.8476\n",
            "  Output5 - P: 0.8142, R: 0.8710, F1: 0.8416\n",
            "  Output6 - P: 0.8594, R: 0.8929, F1: 0.8758\n",
            "  Output7 - P: 0.7835, R: 0.8575, F1: 0.8189\n",
            "  Output8 - P: 0.8181, R: 0.8658, F1: 0.8413\n",
            "平均值 - P: 0.8153, R: 0.8693, F1: 0.8412\n",
            "--------------------------------------------------\n",
            "正在处理第 3/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.5546, R: 0.7444, F1: 0.6356\n",
            "  Output2 - P: 0.7450, R: 0.8217, F1: 0.7815\n",
            "  Output3 - P: 0.6872, R: 0.7633, F1: 0.7233\n",
            "  Output4 - P: 0.7882, R: 0.8035, F1: 0.7958\n",
            "  Output5 - P: 0.6372, R: 0.7855, F1: 0.7037\n",
            "  Output6 - P: 0.7379, R: 0.7797, F1: 0.7582\n",
            "  Output7 - P: 0.5910, R: 0.7614, F1: 0.6655\n",
            "  Output8 - P: 0.6688, R: 0.8162, F1: 0.7352\n",
            "平均值 - P: 0.6762, R: 0.7845, F1: 0.7248\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7385, R: 0.8841, F1: 0.8048\n",
            "  Output2 - P: 0.8509, R: 0.8978, F1: 0.8737\n",
            "  Output3 - P: 0.8190, R: 0.8862, F1: 0.8513\n",
            "  Output4 - P: 0.8857, R: 0.8906, F1: 0.8881\n",
            "  Output5 - P: 0.8088, R: 0.8960, F1: 0.8502\n",
            "  Output6 - P: 0.8524, R: 0.9004, F1: 0.8758\n",
            "  Output7 - P: 0.7832, R: 0.8766, F1: 0.8273\n",
            "  Output8 - P: 0.8043, R: 0.8896, F1: 0.8448\n",
            "平均值 - P: 0.8179, R: 0.8902, F1: 0.8520\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7133, R: 0.8622, F1: 0.7807\n",
            "  Output2 - P: 0.8424, R: 0.8862, F1: 0.8638\n",
            "  Output3 - P: 0.8103, R: 0.8618, F1: 0.8352\n",
            "  Output4 - P: 0.8675, R: 0.8818, F1: 0.8746\n",
            "  Output5 - P: 0.7599, R: 0.8715, F1: 0.8119\n",
            "  Output6 - P: 0.8374, R: 0.8854, F1: 0.8607\n",
            "  Output7 - P: 0.7454, R: 0.8553, F1: 0.7966\n",
            "  Output8 - P: 0.7925, R: 0.8736, F1: 0.8310\n",
            "平均值 - P: 0.7961, R: 0.8722, F1: 0.8318\n",
            "--------------------------------------------------\n",
            "正在处理第 4/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.5181, R: 0.7842, F1: 0.6240\n",
            "  Output2 - P: 0.6313, R: 0.7608, F1: 0.6900\n",
            "  Output3 - P: 0.6204, R: 0.7688, F1: 0.6867\n",
            "  Output4 - P: 0.6574, R: 0.7660, F1: 0.7075\n",
            "  Output5 - P: 0.6411, R: 0.7502, F1: 0.6913\n",
            "  Output6 - P: 0.6791, R: 0.7717, F1: 0.7224\n",
            "  Output7 - P: 0.5584, R: 0.7603, F1: 0.6439\n",
            "  Output8 - P: 0.6006, R: 0.7600, F1: 0.6709\n",
            "平均值 - P: 0.6133, R: 0.7653, F1: 0.6796\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7271, R: 0.9291, F1: 0.8157\n",
            "  Output2 - P: 0.7897, R: 0.9087, F1: 0.8450\n",
            "  Output3 - P: 0.7894, R: 0.9133, F1: 0.8469\n",
            "  Output4 - P: 0.8040, R: 0.9082, F1: 0.8529\n",
            "  Output5 - P: 0.7933, R: 0.9117, F1: 0.8484\n",
            "  Output6 - P: 0.8302, R: 0.9092, F1: 0.8679\n",
            "  Output7 - P: 0.7582, R: 0.9031, F1: 0.8243\n",
            "  Output8 - P: 0.7750, R: 0.9079, F1: 0.8362\n",
            "平均值 - P: 0.7834, R: 0.9114, F1: 0.8422\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7078, R: 0.8868, F1: 0.7872\n",
            "  Output2 - P: 0.7858, R: 0.8432, F1: 0.8135\n",
            "  Output3 - P: 0.7762, R: 0.8392, F1: 0.8065\n",
            "  Output4 - P: 0.7973, R: 0.8453, F1: 0.8206\n",
            "  Output5 - P: 0.7853, R: 0.8508, F1: 0.8168\n",
            "  Output6 - P: 0.8162, R: 0.8556, F1: 0.8354\n",
            "  Output7 - P: 0.7358, R: 0.8672, F1: 0.7962\n",
            "  Output8 - P: 0.7671, R: 0.8465, F1: 0.8048\n",
            "平均值 - P: 0.7714, R: 0.8543, F1: 0.8101\n",
            "--------------------------------------------------\n",
            "正在处理第 5/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6556, R: 0.8056, F1: 0.7229\n",
            "  Output2 - P: 0.7416, R: 0.7816, F1: 0.7610\n",
            "  Output3 - P: 0.7137, R: 0.8009, F1: 0.7548\n",
            "  Output4 - P: 0.7320, R: 0.7980, F1: 0.7636\n",
            "  Output5 - P: 0.7054, R: 0.8122, F1: 0.7551\n",
            "  Output6 - P: 0.8109, R: 0.8472, F1: 0.8287\n",
            "  Output7 - P: 0.6748, R: 0.8152, F1: 0.7384\n",
            "  Output8 - P: 0.7048, R: 0.7908, F1: 0.7453\n",
            "平均值 - P: 0.7174, R: 0.8064, F1: 0.7587\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7779, R: 0.8670, F1: 0.8200\n",
            "  Output2 - P: 0.8385, R: 0.8550, F1: 0.8467\n",
            "  Output3 - P: 0.8303, R: 0.8661, F1: 0.8478\n",
            "  Output4 - P: 0.8314, R: 0.8610, F1: 0.8459\n",
            "  Output5 - P: 0.8239, R: 0.8701, F1: 0.8464\n",
            "  Output6 - P: 0.8605, R: 0.8867, F1: 0.8734\n",
            "  Output7 - P: 0.8055, R: 0.8644, F1: 0.8339\n",
            "  Output8 - P: 0.8225, R: 0.8578, F1: 0.8398\n",
            "平均值 - P: 0.8238, R: 0.8660, F1: 0.8442\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7312, R: 0.8480, F1: 0.7853\n",
            "  Output2 - P: 0.8232, R: 0.8383, F1: 0.8307\n",
            "  Output3 - P: 0.8020, R: 0.8489, F1: 0.8248\n",
            "  Output4 - P: 0.8027, R: 0.8387, F1: 0.8203\n",
            "  Output5 - P: 0.7970, R: 0.8464, F1: 0.8209\n",
            "  Output6 - P: 0.8355, R: 0.8827, F1: 0.8584\n",
            "  Output7 - P: 0.7791, R: 0.8467, F1: 0.8115\n",
            "  Output8 - P: 0.8057, R: 0.8357, F1: 0.8204\n",
            "平均值 - P: 0.7970, R: 0.8482, F1: 0.8215\n",
            "--------------------------------------------------\n",
            "正在处理第 6/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6605, R: 0.7947, F1: 0.7214\n",
            "  Output2 - P: 0.8380, R: 0.8114, F1: 0.8245\n",
            "  Output3 - P: 0.7516, R: 0.8293, F1: 0.7885\n",
            "  Output4 - P: 0.7387, R: 0.8198, F1: 0.7771\n",
            "  Output5 - P: 0.7056, R: 0.8207, F1: 0.7588\n",
            "  Output6 - P: 0.8247, R: 0.8007, F1: 0.8125\n",
            "  Output7 - P: 0.6941, R: 0.7915, F1: 0.7396\n",
            "  Output8 - P: 0.7662, R: 0.8127, F1: 0.7888\n",
            "平均值 - P: 0.7474, R: 0.8101, F1: 0.7764\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7755, R: 0.8855, F1: 0.8268\n",
            "  Output2 - P: 0.8988, R: 0.8807, F1: 0.8897\n",
            "  Output3 - P: 0.8431, R: 0.8900, F1: 0.8659\n",
            "  Output4 - P: 0.8412, R: 0.8894, F1: 0.8647\n",
            "  Output5 - P: 0.8159, R: 0.8857, F1: 0.8493\n",
            "  Output6 - P: 0.8877, R: 0.8860, F1: 0.8869\n",
            "  Output7 - P: 0.8171, R: 0.8644, F1: 0.8401\n",
            "  Output8 - P: 0.8546, R: 0.8786, F1: 0.8665\n",
            "平均值 - P: 0.8417, R: 0.8825, F1: 0.8612\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7384, R: 0.8678, F1: 0.7979\n",
            "  Output2 - P: 0.8896, R: 0.8706, F1: 0.8800\n",
            "  Output3 - P: 0.8151, R: 0.8699, F1: 0.8416\n",
            "  Output4 - P: 0.8088, R: 0.8655, F1: 0.8362\n",
            "  Output5 - P: 0.7730, R: 0.8678, F1: 0.8176\n",
            "  Output6 - P: 0.8547, R: 0.8713, F1: 0.8629\n",
            "  Output7 - P: 0.7866, R: 0.8498, F1: 0.8170\n",
            "  Output8 - P: 0.8400, R: 0.8640, F1: 0.8518\n",
            "平均值 - P: 0.8133, R: 0.8658, F1: 0.8381\n",
            "--------------------------------------------------\n",
            "正在处理第 7/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6739, R: 0.8495, F1: 0.7516\n",
            "  Output2 - P: 0.7698, R: 0.8213, F1: 0.7947\n",
            "  Output3 - P: 0.7303, R: 0.8058, F1: 0.7662\n",
            "  Output4 - P: 0.7453, R: 0.8288, F1: 0.7848\n",
            "  Output5 - P: 0.7243, R: 0.8371, F1: 0.7766\n",
            "  Output6 - P: 0.8479, R: 0.8886, F1: 0.8678\n",
            "  Output7 - P: 0.6540, R: 0.8156, F1: 0.7259\n",
            "  Output8 - P: 0.6647, R: 0.8059, F1: 0.7286\n",
            "平均值 - P: 0.7263, R: 0.8316, F1: 0.7745\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7910, R: 0.9097, F1: 0.8462\n",
            "  Output2 - P: 0.8589, R: 0.8756, F1: 0.8672\n",
            "  Output3 - P: 0.8373, R: 0.8837, F1: 0.8599\n",
            "  Output4 - P: 0.8408, R: 0.8966, F1: 0.8678\n",
            "  Output5 - P: 0.8373, R: 0.8927, F1: 0.8642\n",
            "  Output6 - P: 0.8927, R: 0.9368, F1: 0.9142\n",
            "  Output7 - P: 0.7963, R: 0.8858, F1: 0.8387\n",
            "  Output8 - P: 0.8078, R: 0.8800, F1: 0.8424\n",
            "平均值 - P: 0.8328, R: 0.8951, F1: 0.8626\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7528, R: 0.8975, F1: 0.8188\n",
            "  Output2 - P: 0.8475, R: 0.8598, F1: 0.8536\n",
            "  Output3 - P: 0.8185, R: 0.8672, F1: 0.8421\n",
            "  Output4 - P: 0.8187, R: 0.8823, F1: 0.8493\n",
            "  Output5 - P: 0.8165, R: 0.8773, F1: 0.8459\n",
            "  Output6 - P: 0.8925, R: 0.9265, F1: 0.9092\n",
            "  Output7 - P: 0.7744, R: 0.8627, F1: 0.8161\n",
            "  Output8 - P: 0.7690, R: 0.8542, F1: 0.8094\n",
            "平均值 - P: 0.8112, R: 0.8784, F1: 0.8430\n",
            "--------------------------------------------------\n",
            "正在处理第 8/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6697, R: 0.8034, F1: 0.7305\n",
            "  Output2 - P: 0.7669, R: 0.7798, F1: 0.7733\n",
            "  Output3 - P: 0.7268, R: 0.7796, F1: 0.7523\n",
            "  Output4 - P: 0.7354, R: 0.7872, F1: 0.7604\n",
            "  Output5 - P: 0.7824, R: 0.7787, F1: 0.7806\n",
            "  Output6 - P: 0.8103, R: 0.8219, F1: 0.8160\n",
            "  Output7 - P: 0.6711, R: 0.7826, F1: 0.7226\n",
            "  Output8 - P: 0.7153, R: 0.7827, F1: 0.7475\n",
            "平均值 - P: 0.7347, R: 0.7895, F1: 0.7604\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7859, R: 0.8706, F1: 0.8261\n",
            "  Output2 - P: 0.8618, R: 0.8666, F1: 0.8642\n",
            "  Output3 - P: 0.8356, R: 0.8608, F1: 0.8480\n",
            "  Output4 - P: 0.8338, R: 0.8593, F1: 0.8464\n",
            "  Output5 - P: 0.8577, R: 0.8658, F1: 0.8618\n",
            "  Output6 - P: 0.8847, R: 0.8880, F1: 0.8864\n",
            "  Output7 - P: 0.8023, R: 0.8604, F1: 0.8304\n",
            "  Output8 - P: 0.8251, R: 0.8597, F1: 0.8421\n",
            "平均值 - P: 0.8359, R: 0.8664, F1: 0.8507\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7486, R: 0.8527, F1: 0.7973\n",
            "  Output2 - P: 0.8433, R: 0.8451, F1: 0.8442\n",
            "  Output3 - P: 0.8195, R: 0.8429, F1: 0.8310\n",
            "  Output4 - P: 0.8196, R: 0.8423, F1: 0.8308\n",
            "  Output5 - P: 0.8378, R: 0.8430, F1: 0.8404\n",
            "  Output6 - P: 0.8565, R: 0.8699, F1: 0.8632\n",
            "  Output7 - P: 0.7755, R: 0.8424, F1: 0.8076\n",
            "  Output8 - P: 0.8074, R: 0.8388, F1: 0.8228\n",
            "平均值 - P: 0.8135, R: 0.8472, F1: 0.8297\n",
            "--------------------------------------------------\n",
            "正在处理第 9/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6742, R: 0.8409, F1: 0.7484\n",
            "  Output2 - P: 0.7741, R: 0.7855, F1: 0.7798\n",
            "  Output3 - P: 0.7787, R: 0.8234, F1: 0.8004\n",
            "  Output4 - P: 0.7547, R: 0.8223, F1: 0.7871\n",
            "  Output5 - P: 0.7218, R: 0.8165, F1: 0.7663\n",
            "  Output6 - P: 0.8048, R: 0.8448, F1: 0.8243\n",
            "  Output7 - P: 0.6411, R: 0.8227, F1: 0.7207\n",
            "  Output8 - P: 0.7558, R: 0.8010, F1: 0.7777\n",
            "平均值 - P: 0.7382, R: 0.8196, F1: 0.7756\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7828, R: 0.8897, F1: 0.8328\n",
            "  Output2 - P: 0.8618, R: 0.8484, F1: 0.8551\n",
            "  Output3 - P: 0.8603, R: 0.8760, F1: 0.8681\n",
            "  Output4 - P: 0.8522, R: 0.8767, F1: 0.8643\n",
            "  Output5 - P: 0.8358, R: 0.8700, F1: 0.8525\n",
            "  Output6 - P: 0.8743, R: 0.8954, F1: 0.8847\n",
            "  Output7 - P: 0.7965, R: 0.8664, F1: 0.8300\n",
            "  Output8 - P: 0.8416, R: 0.8584, F1: 0.8500\n",
            "平均值 - P: 0.8382, R: 0.8726, F1: 0.8547\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7487, R: 0.8706, F1: 0.8050\n",
            "  Output2 - P: 0.8531, R: 0.8218, F1: 0.8371\n",
            "  Output3 - P: 0.8522, R: 0.8554, F1: 0.8538\n",
            "  Output4 - P: 0.8374, R: 0.8646, F1: 0.8508\n",
            "  Output5 - P: 0.8321, R: 0.8472, F1: 0.8396\n",
            "  Output6 - P: 0.8403, R: 0.8771, F1: 0.8583\n",
            "  Output7 - P: 0.7605, R: 0.8487, F1: 0.8022\n",
            "  Output8 - P: 0.8341, R: 0.8349, F1: 0.8345\n",
            "平均值 - P: 0.8198, R: 0.8525, F1: 0.8352\n",
            "--------------------------------------------------\n",
            "正在处理第 10/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6216, R: 0.7894, F1: 0.6955\n",
            "  Output2 - P: 0.7583, R: 0.7619, F1: 0.7601\n",
            "  Output3 - P: 0.7485, R: 0.8009, F1: 0.7738\n",
            "  Output4 - P: 0.7612, R: 0.8178, F1: 0.7885\n",
            "  Output5 - P: 0.7335, R: 0.7967, F1: 0.7638\n",
            "  Output6 - P: 0.7687, R: 0.7736, F1: 0.7712\n",
            "  Output7 - P: 0.6572, R: 0.7695, F1: 0.7089\n",
            "  Output8 - P: 0.7180, R: 0.8134, F1: 0.7627\n",
            "平均值 - P: 0.7209, R: 0.7904, F1: 0.7531\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7612, R: 0.8718, F1: 0.8127\n",
            "  Output2 - P: 0.8716, R: 0.8536, F1: 0.8625\n",
            "  Output3 - P: 0.8462, R: 0.8688, F1: 0.8573\n",
            "  Output4 - P: 0.8528, R: 0.8851, F1: 0.8687\n",
            "  Output5 - P: 0.8427, R: 0.8721, F1: 0.8571\n",
            "  Output6 - P: 0.8590, R: 0.8664, F1: 0.8627\n",
            "  Output7 - P: 0.7995, R: 0.8556, F1: 0.8266\n",
            "  Output8 - P: 0.8314, R: 0.8713, F1: 0.8509\n",
            "平均值 - P: 0.8331, R: 0.8681, F1: 0.8498\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7212, R: 0.8498, F1: 0.7802\n",
            "  Output2 - P: 0.8571, R: 0.8420, F1: 0.8495\n",
            "  Output3 - P: 0.8333, R: 0.8575, F1: 0.8452\n",
            "  Output4 - P: 0.8373, R: 0.8768, F1: 0.8566\n",
            "  Output5 - P: 0.8263, R: 0.8597, F1: 0.8427\n",
            "  Output6 - P: 0.8329, R: 0.8521, F1: 0.8424\n",
            "  Output7 - P: 0.7766, R: 0.8426, F1: 0.8082\n",
            "  Output8 - P: 0.8141, R: 0.8542, F1: 0.8337\n",
            "平均值 - P: 0.8124, R: 0.8543, F1: 0.8323\n",
            "--------------------------------------------------\n",
            "正在处理第 11/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6307, R: 0.8039, F1: 0.7068\n",
            "  Output2 - P: 0.7738, R: 0.7922, F1: 0.7829\n",
            "  Output3 - P: 0.7612, R: 0.8323, F1: 0.7952\n",
            "  Output4 - P: 0.7444, R: 0.8307, F1: 0.7852\n",
            "  Output5 - P: 0.7099, R: 0.8045, F1: 0.7543\n",
            "  Output6 - P: 0.8138, R: 0.8242, F1: 0.8189\n",
            "  Output7 - P: 0.6324, R: 0.7824, F1: 0.6994\n",
            "  Output8 - P: 0.7027, R: 0.8141, F1: 0.7543\n",
            "平均值 - P: 0.7211, R: 0.8105, F1: 0.7621\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7680, R: 0.8896, F1: 0.8244\n",
            "  Output2 - P: 0.8731, R: 0.8619, F1: 0.8675\n",
            "  Output3 - P: 0.8528, R: 0.8894, F1: 0.8707\n",
            "  Output4 - P: 0.8401, R: 0.8906, F1: 0.8646\n",
            "  Output5 - P: 0.8319, R: 0.8760, F1: 0.8533\n",
            "  Output6 - P: 0.8782, R: 0.8899, F1: 0.8840\n",
            "  Output7 - P: 0.7909, R: 0.8661, F1: 0.8268\n",
            "  Output8 - P: 0.8215, R: 0.8723, F1: 0.8461\n",
            "平均值 - P: 0.8321, R: 0.8795, F1: 0.8547\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7269, R: 0.8706, F1: 0.7923\n",
            "  Output2 - P: 0.8630, R: 0.8514, F1: 0.8571\n",
            "  Output3 - P: 0.8399, R: 0.8800, F1: 0.8595\n",
            "  Output4 - P: 0.8236, R: 0.8803, F1: 0.8510\n",
            "  Output5 - P: 0.8105, R: 0.8630, F1: 0.8359\n",
            "  Output6 - P: 0.8636, R: 0.8850, F1: 0.8742\n",
            "  Output7 - P: 0.7540, R: 0.8434, F1: 0.7962\n",
            "  Output8 - P: 0.8058, R: 0.8524, F1: 0.8285\n",
            "平均值 - P: 0.8109, R: 0.8658, F1: 0.8368\n",
            "--------------------------------------------------\n",
            "正在处理第 12/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6446, R: 0.8379, F1: 0.7286\n",
            "  Output2 - P: 0.7585, R: 0.8147, F1: 0.7856\n",
            "  Output3 - P: 0.7341, R: 0.8161, F1: 0.7729\n",
            "  Output4 - P: 0.7332, R: 0.8440, F1: 0.7847\n",
            "  Output5 - P: 0.7328, R: 0.8395, F1: 0.7825\n",
            "  Output6 - P: 0.8271, R: 0.8409, F1: 0.8340\n",
            "  Output7 - P: 0.6792, R: 0.8181, F1: 0.7422\n",
            "  Output8 - P: 0.7338, R: 0.8187, F1: 0.7739\n",
            "平均值 - P: 0.7304, R: 0.8287, F1: 0.7756\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7632, R: 0.8875, F1: 0.8206\n",
            "  Output2 - P: 0.8604, R: 0.8787, F1: 0.8695\n",
            "  Output3 - P: 0.8371, R: 0.8761, F1: 0.8562\n",
            "  Output4 - P: 0.8331, R: 0.8865, F1: 0.8590\n",
            "  Output5 - P: 0.8359, R: 0.8820, F1: 0.8584\n",
            "  Output6 - P: 0.8772, R: 0.8928, F1: 0.8849\n",
            "  Output7 - P: 0.8066, R: 0.8767, F1: 0.8402\n",
            "  Output8 - P: 0.8402, R: 0.8750, F1: 0.8572\n",
            "平均值 - P: 0.8317, R: 0.8819, F1: 0.8558\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7453, R: 0.8754, F1: 0.8052\n",
            "  Output2 - P: 0.8405, R: 0.8708, F1: 0.8554\n",
            "  Output3 - P: 0.8174, R: 0.8614, F1: 0.8388\n",
            "  Output4 - P: 0.8132, R: 0.8708, F1: 0.8410\n",
            "  Output5 - P: 0.8194, R: 0.8695, F1: 0.8437\n",
            "  Output6 - P: 0.8625, R: 0.8922, F1: 0.8771\n",
            "  Output7 - P: 0.7825, R: 0.8579, F1: 0.8185\n",
            "  Output8 - P: 0.8226, R: 0.8565, F1: 0.8392\n",
            "平均值 - P: 0.8129, R: 0.8693, F1: 0.8399\n",
            "--------------------------------------------------\n",
            "正在处理第 13/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6718, R: 0.8335, F1: 0.7440\n",
            "  Output2 - P: 0.7358, R: 0.7625, F1: 0.7489\n",
            "  Output3 - P: 0.6928, R: 0.7736, F1: 0.7310\n",
            "  Output4 - P: 0.6680, R: 0.7674, F1: 0.7143\n",
            "  Output5 - P: 0.6832, R: 0.7790, F1: 0.7279\n",
            "  Output6 - P: 0.7949, R: 0.8439, F1: 0.8187\n",
            "  Output7 - P: 0.6360, R: 0.7780, F1: 0.6999\n",
            "  Output8 - P: 0.7227, R: 0.7785, F1: 0.7496\n",
            "平均值 - P: 0.7007, R: 0.7895, F1: 0.7418\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7807, R: 0.8812, F1: 0.8279\n",
            "  Output2 - P: 0.8413, R: 0.8400, F1: 0.8407\n",
            "  Output3 - P: 0.8170, R: 0.8478, F1: 0.8321\n",
            "  Output4 - P: 0.8091, R: 0.8433, F1: 0.8258\n",
            "  Output5 - P: 0.8090, R: 0.8489, F1: 0.8284\n",
            "  Output6 - P: 0.8549, R: 0.8856, F1: 0.8700\n",
            "  Output7 - P: 0.7943, R: 0.8483, F1: 0.8204\n",
            "  Output8 - P: 0.8355, R: 0.8527, F1: 0.8440\n",
            "平均值 - P: 0.8177, R: 0.8560, F1: 0.8362\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7428, R: 0.8529, F1: 0.7941\n",
            "  Output2 - P: 0.8359, R: 0.8043, F1: 0.8198\n",
            "  Output3 - P: 0.7967, R: 0.8168, F1: 0.8066\n",
            "  Output4 - P: 0.7849, R: 0.8066, F1: 0.7956\n",
            "  Output5 - P: 0.7863, R: 0.8068, F1: 0.7964\n",
            "  Output6 - P: 0.8128, R: 0.8566, F1: 0.8341\n",
            "  Output7 - P: 0.7662, R: 0.8104, F1: 0.7877\n",
            "  Output8 - P: 0.8211, R: 0.8105, F1: 0.8158\n",
            "平均值 - P: 0.7933, R: 0.8206, F1: 0.8063\n",
            "--------------------------------------------------\n",
            "正在处理第 14/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.5939, R: 0.7948, F1: 0.6798\n",
            "  Output2 - P: 0.7651, R: 0.7881, F1: 0.7764\n",
            "  Output3 - P: 0.7254, R: 0.6970, F1: 0.7109\n",
            "  Output4 - P: 0.7604, R: 0.7981, F1: 0.7788\n",
            "  Output5 - P: 0.6721, R: 0.8072, F1: 0.7335\n",
            "  Output6 - P: 0.7657, R: 0.7943, F1: 0.7798\n",
            "  Output7 - P: 0.6294, R: 0.7922, F1: 0.7015\n",
            "  Output8 - P: 0.7131, R: 0.7944, F1: 0.7516\n",
            "平均值 - P: 0.7031, R: 0.7833, F1: 0.7390\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7634, R: 0.8854, F1: 0.8199\n",
            "  Output2 - P: 0.8832, R: 0.8786, F1: 0.8809\n",
            "  Output3 - P: 0.8706, R: 0.8399, F1: 0.8550\n",
            "  Output4 - P: 0.8567, R: 0.8764, F1: 0.8665\n",
            "  Output5 - P: 0.8159, R: 0.8746, F1: 0.8442\n",
            "  Output6 - P: 0.8595, R: 0.8844, F1: 0.8718\n",
            "  Output7 - P: 0.7890, R: 0.8629, F1: 0.8243\n",
            "  Output8 - P: 0.8373, R: 0.8709, F1: 0.8538\n",
            "平均值 - P: 0.8345, R: 0.8716, F1: 0.8520\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7177, R: 0.8714, F1: 0.7872\n",
            "  Output2 - P: 0.8682, R: 0.8696, F1: 0.8689\n",
            "  Output3 - P: 0.8436, R: 0.8329, F1: 0.8382\n",
            "  Output4 - P: 0.8420, R: 0.8737, F1: 0.8575\n",
            "  Output5 - P: 0.7787, R: 0.8627, F1: 0.8186\n",
            "  Output6 - P: 0.8449, R: 0.8768, F1: 0.8606\n",
            "  Output7 - P: 0.7545, R: 0.8564, F1: 0.8022\n",
            "  Output8 - P: 0.8235, R: 0.8629, F1: 0.8427\n",
            "平均值 - P: 0.8091, R: 0.8633, F1: 0.8345\n",
            "--------------------------------------------------\n",
            "正在处理第 15/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6247, R: 0.7727, F1: 0.6909\n",
            "  Output2 - P: 0.7763, R: 0.7936, F1: 0.7848\n",
            "  Output3 - P: 0.7361, R: 0.7868, F1: 0.7606\n",
            "  Output4 - P: 0.7397, R: 0.8105, F1: 0.7735\n",
            "  Output5 - P: 0.7440, R: 0.8096, F1: 0.7754\n",
            "  Output6 - P: 0.7860, R: 0.7966, F1: 0.7913\n",
            "  Output7 - P: 0.6682, R: 0.7739, F1: 0.7172\n",
            "  Output8 - P: 0.7049, R: 0.7786, F1: 0.7399\n",
            "平均值 - P: 0.7225, R: 0.7903, F1: 0.7542\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7623, R: 0.8619, F1: 0.8090\n",
            "  Output2 - P: 0.8564, R: 0.8659, F1: 0.8611\n",
            "  Output3 - P: 0.8350, R: 0.8655, F1: 0.8500\n",
            "  Output4 - P: 0.8347, R: 0.8743, F1: 0.8540\n",
            "  Output5 - P: 0.8404, R: 0.8725, F1: 0.8561\n",
            "  Output6 - P: 0.8734, R: 0.8751, F1: 0.8743\n",
            "  Output7 - P: 0.8067, R: 0.8542, F1: 0.8298\n",
            "  Output8 - P: 0.8292, R: 0.8586, F1: 0.8437\n",
            "平均值 - P: 0.8298, R: 0.8660, F1: 0.8472\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7186, R: 0.8490, F1: 0.7784\n",
            "  Output2 - P: 0.8498, R: 0.8508, F1: 0.8503\n",
            "  Output3 - P: 0.8172, R: 0.8570, F1: 0.8366\n",
            "  Output4 - P: 0.8140, R: 0.8639, F1: 0.8382\n",
            "  Output5 - P: 0.8224, R: 0.8616, F1: 0.8416\n",
            "  Output6 - P: 0.8374, R: 0.8637, F1: 0.8503\n",
            "  Output7 - P: 0.7768, R: 0.8393, F1: 0.8068\n",
            "  Output8 - P: 0.8078, R: 0.8418, F1: 0.8245\n",
            "平均值 - P: 0.8055, R: 0.8534, F1: 0.8283\n",
            "--------------------------------------------------\n",
            "正在处理第 16/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6505, R: 0.8116, F1: 0.7222\n",
            "  Output2 - P: 0.7414, R: 0.7607, F1: 0.7509\n",
            "  Output3 - P: 0.7202, R: 0.8051, F1: 0.7603\n",
            "  Output4 - P: 0.7624, R: 0.8248, F1: 0.7924\n",
            "  Output5 - P: 0.7041, R: 0.8063, F1: 0.7518\n",
            "  Output6 - P: 0.7944, R: 0.8347, F1: 0.8141\n",
            "  Output7 - P: 0.6565, R: 0.7836, F1: 0.7144\n",
            "  Output8 - P: 0.7262, R: 0.8326, F1: 0.7757\n",
            "平均值 - P: 0.7195, R: 0.8074, F1: 0.7602\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7629, R: 0.8792, F1: 0.8169\n",
            "  Output2 - P: 0.8467, R: 0.8431, F1: 0.8449\n",
            "  Output3 - P: 0.8257, R: 0.8718, F1: 0.8482\n",
            "  Output4 - P: 0.8504, R: 0.8705, F1: 0.8603\n",
            "  Output5 - P: 0.8244, R: 0.8711, F1: 0.8471\n",
            "  Output6 - P: 0.8759, R: 0.8887, F1: 0.8823\n",
            "  Output7 - P: 0.7935, R: 0.8594, F1: 0.8251\n",
            "  Output8 - P: 0.8318, R: 0.8762, F1: 0.8534\n",
            "平均值 - P: 0.8264, R: 0.8700, F1: 0.8473\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7137, R: 0.8681, F1: 0.7834\n",
            "  Output2 - P: 0.8345, R: 0.8241, F1: 0.8293\n",
            "  Output3 - P: 0.7985, R: 0.8562, F1: 0.8263\n",
            "  Output4 - P: 0.8320, R: 0.8589, F1: 0.8452\n",
            "  Output5 - P: 0.7976, R: 0.8590, F1: 0.8272\n",
            "  Output6 - P: 0.8457, R: 0.8750, F1: 0.8601\n",
            "  Output7 - P: 0.7717, R: 0.8441, F1: 0.8063\n",
            "  Output8 - P: 0.8101, R: 0.8582, F1: 0.8335\n",
            "平均值 - P: 0.8005, R: 0.8554, F1: 0.8264\n",
            "--------------------------------------------------\n",
            "正在处理第 17/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.5837, R: 0.8353, F1: 0.6872\n",
            "  Output2 - P: 0.7523, R: 0.8356, F1: 0.7918\n",
            "  Output3 - P: 0.7050, R: 0.8195, F1: 0.7579\n",
            "  Output4 - P: 0.7007, R: 0.8204, F1: 0.7558\n",
            "  Output5 - P: 0.6759, R: 0.8288, F1: 0.7446\n",
            "  Output6 - P: 0.7570, R: 0.8457, F1: 0.7989\n",
            "  Output7 - P: 0.6287, R: 0.8200, F1: 0.7117\n",
            "  Output8 - P: 0.7230, R: 0.8324, F1: 0.7738\n",
            "平均值 - P: 0.6908, R: 0.8297, F1: 0.7527\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7536, R: 0.8957, F1: 0.8185\n",
            "  Output2 - P: 0.8574, R: 0.8926, F1: 0.8746\n",
            "  Output3 - P: 0.8154, R: 0.8894, F1: 0.8508\n",
            "  Output4 - P: 0.8234, R: 0.8941, F1: 0.8573\n",
            "  Output5 - P: 0.8078, R: 0.8948, F1: 0.8491\n",
            "  Output6 - P: 0.8473, R: 0.9049, F1: 0.8752\n",
            "  Output7 - P: 0.7929, R: 0.8802, F1: 0.8343\n",
            "  Output8 - P: 0.8276, R: 0.8911, F1: 0.8582\n",
            "平均值 - P: 0.8157, R: 0.8928, F1: 0.8522\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7089, R: 0.8689, F1: 0.7808\n",
            "  Output2 - P: 0.8415, R: 0.8635, F1: 0.8524\n",
            "  Output3 - P: 0.7840, R: 0.8647, F1: 0.8224\n",
            "  Output4 - P: 0.7973, R: 0.8654, F1: 0.8300\n",
            "  Output5 - P: 0.7703, R: 0.8701, F1: 0.8171\n",
            "  Output6 - P: 0.8209, R: 0.8858, F1: 0.8521\n",
            "  Output7 - P: 0.7673, R: 0.8616, F1: 0.8118\n",
            "  Output8 - P: 0.8133, R: 0.8626, F1: 0.8372\n",
            "平均值 - P: 0.7879, R: 0.8678, F1: 0.8255\n",
            "--------------------------------------------------\n",
            "正在处理第 18/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.5812, R: 0.7585, F1: 0.6581\n",
            "  Output2 - P: 0.7655, R: 0.7909, F1: 0.7780\n",
            "  Output3 - P: 0.7142, R: 0.7651, F1: 0.7388\n",
            "  Output4 - P: 0.6858, R: 0.7778, F1: 0.7289\n",
            "  Output5 - P: 0.6430, R: 0.7319, F1: 0.6846\n",
            "  Output6 - P: 0.7250, R: 0.7564, F1: 0.7404\n",
            "  Output7 - P: 0.6260, R: 0.7345, F1: 0.6759\n",
            "  Output8 - P: 0.6903, R: 0.7988, F1: 0.7406\n",
            "平均值 - P: 0.6789, R: 0.7642, F1: 0.7182\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7548, R: 0.8667, F1: 0.8069\n",
            "  Output2 - P: 0.8629, R: 0.8640, F1: 0.8634\n",
            "  Output3 - P: 0.8245, R: 0.8666, F1: 0.8450\n",
            "  Output4 - P: 0.8193, R: 0.8683, F1: 0.8431\n",
            "  Output5 - P: 0.8002, R: 0.8594, F1: 0.8287\n",
            "  Output6 - P: 0.8258, R: 0.8751, F1: 0.8497\n",
            "  Output7 - P: 0.7827, R: 0.8539, F1: 0.8167\n",
            "  Output8 - P: 0.8144, R: 0.8696, F1: 0.8411\n",
            "平均值 - P: 0.8106, R: 0.8654, F1: 0.8368\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7241, R: 0.8493, F1: 0.7817\n",
            "  Output2 - P: 0.8494, R: 0.8450, F1: 0.8472\n",
            "  Output3 - P: 0.8127, R: 0.8473, F1: 0.8296\n",
            "  Output4 - P: 0.8003, R: 0.8387, F1: 0.8191\n",
            "  Output5 - P: 0.7790, R: 0.8312, F1: 0.8043\n",
            "  Output6 - P: 0.7973, R: 0.8509, F1: 0.8232\n",
            "  Output7 - P: 0.7467, R: 0.8279, F1: 0.7852\n",
            "  Output8 - P: 0.8010, R: 0.8440, F1: 0.8219\n",
            "平均值 - P: 0.7888, R: 0.8418, F1: 0.8140\n",
            "--------------------------------------------------\n",
            "正在处理第 19/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.5671, R: 0.7631, F1: 0.6506\n",
            "  Output2 - P: 0.7201, R: 0.7684, F1: 0.7434\n",
            "  Output3 - P: 0.6457, R: 0.7699, F1: 0.7023\n",
            "  Output4 - P: 0.6585, R: 0.7753, F1: 0.7122\n",
            "  Output5 - P: 0.6707, R: 0.7848, F1: 0.7233\n",
            "  Output6 - P: 0.7040, R: 0.7765, F1: 0.7385\n",
            "  Output7 - P: 0.5947, R: 0.7673, F1: 0.6701\n",
            "  Output8 - P: 0.6181, R: 0.7370, F1: 0.6723\n",
            "平均值 - P: 0.6474, R: 0.7678, F1: 0.7016\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7529, R: 0.8697, F1: 0.8071\n",
            "  Output2 - P: 0.8242, R: 0.8674, F1: 0.8453\n",
            "  Output3 - P: 0.7968, R: 0.8592, F1: 0.8268\n",
            "  Output4 - P: 0.8030, R: 0.8628, F1: 0.8318\n",
            "  Output5 - P: 0.8033, R: 0.8717, F1: 0.8361\n",
            "  Output6 - P: 0.8262, R: 0.8733, F1: 0.8491\n",
            "  Output7 - P: 0.7779, R: 0.8676, F1: 0.8203\n",
            "  Output8 - P: 0.7913, R: 0.8545, F1: 0.8216\n",
            "平均值 - P: 0.7970, R: 0.8658, F1: 0.8298\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7267, R: 0.8482, F1: 0.7828\n",
            "  Output2 - P: 0.8151, R: 0.8425, F1: 0.8286\n",
            "  Output3 - P: 0.7835, R: 0.8347, F1: 0.8083\n",
            "  Output4 - P: 0.7906, R: 0.8508, F1: 0.8196\n",
            "  Output5 - P: 0.7951, R: 0.8494, F1: 0.8214\n",
            "  Output6 - P: 0.8131, R: 0.8494, F1: 0.8308\n",
            "  Output7 - P: 0.7546, R: 0.8423, F1: 0.7961\n",
            "  Output8 - P: 0.7728, R: 0.8184, F1: 0.7949\n",
            "平均值 - P: 0.7814, R: 0.8420, F1: 0.8103\n",
            "--------------------------------------------------\n",
            "正在处理第 20/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6223, R: 0.8285, F1: 0.7107\n",
            "  Output2 - P: 0.7584, R: 0.7845, F1: 0.7712\n",
            "  Output3 - P: 0.6934, R: 0.8087, F1: 0.7467\n",
            "  Output4 - P: 0.7391, R: 0.8241, F1: 0.7793\n",
            "  Output5 - P: 0.7276, R: 0.8025, F1: 0.7632\n",
            "  Output6 - P: 0.8132, R: 0.8275, F1: 0.8203\n",
            "  Output7 - P: 0.6298, R: 0.7738, F1: 0.6944\n",
            "  Output8 - P: 0.7095, R: 0.8096, F1: 0.7562\n",
            "平均值 - P: 0.7117, R: 0.8074, F1: 0.7553\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7697, R: 0.8890, F1: 0.8250\n",
            "  Output2 - P: 0.8680, R: 0.8643, F1: 0.8662\n",
            "  Output3 - P: 0.8252, R: 0.8761, F1: 0.8499\n",
            "  Output4 - P: 0.8395, R: 0.8751, F1: 0.8569\n",
            "  Output5 - P: 0.8245, R: 0.8665, F1: 0.8450\n",
            "  Output6 - P: 0.8707, R: 0.8862, F1: 0.8784\n",
            "  Output7 - P: 0.7847, R: 0.8533, F1: 0.8176\n",
            "  Output8 - P: 0.8201, R: 0.8724, F1: 0.8454\n",
            "平均值 - P: 0.8253, R: 0.8728, F1: 0.8480\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7304, R: 0.8687, F1: 0.7935\n",
            "  Output2 - P: 0.8498, R: 0.8493, F1: 0.8495\n",
            "  Output3 - P: 0.8038, R: 0.8650, F1: 0.8333\n",
            "  Output4 - P: 0.8195, R: 0.8640, F1: 0.8412\n",
            "  Output5 - P: 0.8098, R: 0.8525, F1: 0.8306\n",
            "  Output6 - P: 0.8570, R: 0.8726, F1: 0.8647\n",
            "  Output7 - P: 0.7536, R: 0.8418, F1: 0.7953\n",
            "  Output8 - P: 0.8024, R: 0.8522, F1: 0.8266\n",
            "平均值 - P: 0.8033, R: 0.8583, F1: 0.8293\n",
            "--------------------------------------------------\n",
            "正在处理第 21/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6351, R: 0.7597, F1: 0.6918\n",
            "  Output2 - P: 0.7560, R: 0.7550, F1: 0.7555\n",
            "  Output3 - P: 0.7238, R: 0.7800, F1: 0.7508\n",
            "  Output4 - P: 0.7239, R: 0.7758, F1: 0.7490\n",
            "  Output5 - P: 0.6859, R: 0.7706, F1: 0.7258\n",
            "  Output6 - P: 0.7371, R: 0.7701, F1: 0.7532\n",
            "  Output7 - P: 0.6398, R: 0.7730, F1: 0.7001\n",
            "  Output8 - P: 0.6993, R: 0.7761, F1: 0.7357\n",
            "平均值 - P: 0.7001, R: 0.7700, F1: 0.7327\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7735, R: 0.8466, F1: 0.8084\n",
            "  Output2 - P: 0.8662, R: 0.8472, F1: 0.8566\n",
            "  Output3 - P: 0.8292, R: 0.8614, F1: 0.8450\n",
            "  Output4 - P: 0.8317, R: 0.8559, F1: 0.8436\n",
            "  Output5 - P: 0.8186, R: 0.8561, F1: 0.8369\n",
            "  Output6 - P: 0.8466, R: 0.8540, F1: 0.8503\n",
            "  Output7 - P: 0.7932, R: 0.8498, F1: 0.8205\n",
            "  Output8 - P: 0.8156, R: 0.8495, F1: 0.8322\n",
            "平均值 - P: 0.8218, R: 0.8526, F1: 0.8367\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7480, R: 0.8293, F1: 0.7865\n",
            "  Output2 - P: 0.8396, R: 0.8250, F1: 0.8323\n",
            "  Output3 - P: 0.8085, R: 0.8415, F1: 0.8247\n",
            "  Output4 - P: 0.8118, R: 0.8388, F1: 0.8251\n",
            "  Output5 - P: 0.7940, R: 0.8367, F1: 0.8148\n",
            "  Output6 - P: 0.8238, R: 0.8349, F1: 0.8293\n",
            "  Output7 - P: 0.7594, R: 0.8305, F1: 0.7934\n",
            "  Output8 - P: 0.7914, R: 0.8196, F1: 0.8052\n",
            "平均值 - P: 0.7971, R: 0.8320, F1: 0.8139\n",
            "--------------------------------------------------\n",
            "正在处理第 22/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.5968, R: 0.7199, F1: 0.6526\n",
            "  Output2 - P: 0.7579, R: 0.7816, F1: 0.7696\n",
            "  Output3 - P: 0.6719, R: 0.7669, F1: 0.7163\n",
            "  Output4 - P: 0.6798, R: 0.7714, F1: 0.7227\n",
            "  Output5 - P: 0.7231, R: 0.7852, F1: 0.7529\n",
            "  Output6 - P: 0.7523, R: 0.7677, F1: 0.7599\n",
            "  Output7 - P: 0.6257, R: 0.7618, F1: 0.6871\n",
            "  Output8 - P: 0.6941, R: 0.7959, F1: 0.7415\n",
            "平均值 - P: 0.6877, R: 0.7688, F1: 0.7253\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7511, R: 0.8390, F1: 0.7926\n",
            "  Output2 - P: 0.8510, R: 0.8680, F1: 0.8594\n",
            "  Output3 - P: 0.8202, R: 0.8617, F1: 0.8404\n",
            "  Output4 - P: 0.8182, R: 0.8635, F1: 0.8402\n",
            "  Output5 - P: 0.8343, R: 0.8653, F1: 0.8495\n",
            "  Output6 - P: 0.8503, R: 0.8630, F1: 0.8566\n",
            "  Output7 - P: 0.7934, R: 0.8525, F1: 0.8219\n",
            "  Output8 - P: 0.8255, R: 0.8680, F1: 0.8462\n",
            "平均值 - P: 0.8180, R: 0.8601, F1: 0.8384\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7139, R: 0.8232, F1: 0.7647\n",
            "  Output2 - P: 0.8340, R: 0.8512, F1: 0.8425\n",
            "  Output3 - P: 0.7877, R: 0.8447, F1: 0.8152\n",
            "  Output4 - P: 0.7953, R: 0.8467, F1: 0.8202\n",
            "  Output5 - P: 0.8139, R: 0.8509, F1: 0.8320\n",
            "  Output6 - P: 0.8214, R: 0.8460, F1: 0.8335\n",
            "  Output7 - P: 0.7599, R: 0.8368, F1: 0.7965\n",
            "  Output8 - P: 0.7972, R: 0.8487, F1: 0.8222\n",
            "平均值 - P: 0.7904, R: 0.8435, F1: 0.8158\n",
            "--------------------------------------------------\n",
            "正在处理第 23/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6165, R: 0.7510, F1: 0.6772\n",
            "  Output2 - P: 0.8032, R: 0.8069, F1: 0.8051\n",
            "  Output3 - P: 0.7464, R: 0.8145, F1: 0.7790\n",
            "  Output4 - P: 0.7670, R: 0.8043, F1: 0.7852\n",
            "  Output5 - P: 0.7913, R: 0.8288, F1: 0.8096\n",
            "  Output6 - P: 0.8261, R: 0.7817, F1: 0.8033\n",
            "  Output7 - P: 0.6669, R: 0.7716, F1: 0.7154\n",
            "  Output8 - P: 0.7238, R: 0.8024, F1: 0.7611\n",
            "平均值 - P: 0.7427, R: 0.7952, F1: 0.7670\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7449, R: 0.8502, F1: 0.7941\n",
            "  Output2 - P: 0.8858, R: 0.8754, F1: 0.8806\n",
            "  Output3 - P: 0.8434, R: 0.8746, F1: 0.8587\n",
            "  Output4 - P: 0.8503, R: 0.8744, F1: 0.8622\n",
            "  Output5 - P: 0.8657, R: 0.8902, F1: 0.8778\n",
            "  Output6 - P: 0.8974, R: 0.8765, F1: 0.8868\n",
            "  Output7 - P: 0.8048, R: 0.8584, F1: 0.8307\n",
            "  Output8 - P: 0.8361, R: 0.8673, F1: 0.8514\n",
            "平均值 - P: 0.8410, R: 0.8709, F1: 0.8553\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7159, R: 0.8462, F1: 0.7756\n",
            "  Output2 - P: 0.8776, R: 0.8647, F1: 0.8711\n",
            "  Output3 - P: 0.8289, R: 0.8602, F1: 0.8442\n",
            "  Output4 - P: 0.8309, R: 0.8585, F1: 0.8445\n",
            "  Output5 - P: 0.8433, R: 0.8748, F1: 0.8588\n",
            "  Output6 - P: 0.8827, R: 0.8629, F1: 0.8727\n",
            "  Output7 - P: 0.7853, R: 0.8412, F1: 0.8123\n",
            "  Output8 - P: 0.8173, R: 0.8526, F1: 0.8346\n",
            "平均值 - P: 0.8227, R: 0.8576, F1: 0.8392\n",
            "--------------------------------------------------\n",
            "正在处理第 24/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6290, R: 0.7501, F1: 0.6843\n",
            "  Output2 - P: 0.7637, R: 0.7534, F1: 0.7586\n",
            "  Output3 - P: 0.7389, R: 0.7786, F1: 0.7582\n",
            "  Output4 - P: 0.6986, R: 0.7813, F1: 0.7376\n",
            "  Output5 - P: 0.7588, R: 0.7674, F1: 0.7631\n",
            "  Output6 - P: 0.7574, R: 0.7613, F1: 0.7593\n",
            "  Output7 - P: 0.6480, R: 0.7581, F1: 0.6988\n",
            "  Output8 - P: 0.7202, R: 0.7855, F1: 0.7515\n",
            "平均值 - P: 0.7143, R: 0.7670, F1: 0.7389\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7666, R: 0.8543, F1: 0.8081\n",
            "  Output2 - P: 0.8591, R: 0.8489, F1: 0.8540\n",
            "  Output3 - P: 0.8382, R: 0.8637, F1: 0.8508\n",
            "  Output4 - P: 0.8214, R: 0.8623, F1: 0.8414\n",
            "  Output5 - P: 0.8521, R: 0.8596, F1: 0.8558\n",
            "  Output6 - P: 0.8450, R: 0.8610, F1: 0.8529\n",
            "  Output7 - P: 0.8016, R: 0.8569, F1: 0.8283\n",
            "  Output8 - P: 0.8313, R: 0.8607, F1: 0.8457\n",
            "平均值 - P: 0.8269, R: 0.8584, F1: 0.8421\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7216, R: 0.8375, F1: 0.7752\n",
            "  Output2 - P: 0.8479, R: 0.8280, F1: 0.8378\n",
            "  Output3 - P: 0.8212, R: 0.8463, F1: 0.8336\n",
            "  Output4 - P: 0.7980, R: 0.8445, F1: 0.8206\n",
            "  Output5 - P: 0.8313, R: 0.8452, F1: 0.8382\n",
            "  Output6 - P: 0.8227, R: 0.8461, F1: 0.8342\n",
            "  Output7 - P: 0.7702, R: 0.8328, F1: 0.8003\n",
            "  Output8 - P: 0.8143, R: 0.8380, F1: 0.8260\n",
            "平均值 - P: 0.8034, R: 0.8398, F1: 0.8207\n",
            "--------------------------------------------------\n",
            "正在处理第 25/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6458, R: 0.7965, F1: 0.7133\n",
            "  Output2 - P: 0.7980, R: 0.8006, F1: 0.7993\n",
            "  Output3 - P: 0.7443, R: 0.8150, F1: 0.7780\n",
            "  Output4 - P: 0.6923, R: 0.7836, F1: 0.7351\n",
            "  Output5 - P: 0.7202, R: 0.8049, F1: 0.7602\n",
            "  Output6 - P: 0.7865, R: 0.7878, F1: 0.7872\n",
            "  Output7 - P: 0.6589, R: 0.7851, F1: 0.7165\n",
            "  Output8 - P: 0.6816, R: 0.8066, F1: 0.7389\n",
            "平均值 - P: 0.7160, R: 0.7975, F1: 0.7536\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7788, R: 0.8826, F1: 0.8275\n",
            "  Output2 - P: 0.8900, R: 0.8825, F1: 0.8862\n",
            "  Output3 - P: 0.8497, R: 0.8826, F1: 0.8658\n",
            "  Output4 - P: 0.8283, R: 0.8684, F1: 0.8478\n",
            "  Output5 - P: 0.8447, R: 0.8834, F1: 0.8636\n",
            "  Output6 - P: 0.8653, R: 0.8754, F1: 0.8704\n",
            "  Output7 - P: 0.8113, R: 0.8682, F1: 0.8388\n",
            "  Output8 - P: 0.8220, R: 0.8758, F1: 0.8480\n",
            "平均值 - P: 0.8363, R: 0.8774, F1: 0.8560\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7413, R: 0.8649, F1: 0.7984\n",
            "  Output2 - P: 0.8788, R: 0.8678, F1: 0.8733\n",
            "  Output3 - P: 0.8160, R: 0.8641, F1: 0.8394\n",
            "  Output4 - P: 0.7996, R: 0.8488, F1: 0.8235\n",
            "  Output5 - P: 0.8229, R: 0.8639, F1: 0.8429\n",
            "  Output6 - P: 0.8292, R: 0.8598, F1: 0.8442\n",
            "  Output7 - P: 0.7753, R: 0.8514, F1: 0.8116\n",
            "  Output8 - P: 0.7871, R: 0.8509, F1: 0.8178\n",
            "平均值 - P: 0.8063, R: 0.8590, F1: 0.8314\n",
            "--------------------------------------------------\n",
            "正在处理第 26/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6457, R: 0.7767, F1: 0.7052\n",
            "  Output2 - P: 0.7838, R: 0.7770, F1: 0.7804\n",
            "  Output3 - P: 0.7510, R: 0.8183, F1: 0.7832\n",
            "  Output4 - P: 0.7564, R: 0.7958, F1: 0.7756\n",
            "  Output5 - P: 0.7227, R: 0.8059, F1: 0.7621\n",
            "  Output6 - P: 0.8241, R: 0.7941, F1: 0.8088\n",
            "  Output7 - P: 0.6553, R: 0.7902, F1: 0.7164\n",
            "  Output8 - P: 0.7343, R: 0.8070, F1: 0.7690\n",
            "平均值 - P: 0.7342, R: 0.7956, F1: 0.7626\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7682, R: 0.8665, F1: 0.8144\n",
            "  Output2 - P: 0.8828, R: 0.8630, F1: 0.8728\n",
            "  Output3 - P: 0.8457, R: 0.8798, F1: 0.8624\n",
            "  Output4 - P: 0.8486, R: 0.8685, F1: 0.8584\n",
            "  Output5 - P: 0.8346, R: 0.8770, F1: 0.8553\n",
            "  Output6 - P: 0.8857, R: 0.8798, F1: 0.8827\n",
            "  Output7 - P: 0.8017, R: 0.8706, F1: 0.8347\n",
            "  Output8 - P: 0.8399, R: 0.8730, F1: 0.8561\n",
            "平均值 - P: 0.8384, R: 0.8723, F1: 0.8546\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7282, R: 0.8486, F1: 0.7838\n",
            "  Output2 - P: 0.8686, R: 0.8510, F1: 0.8597\n",
            "  Output3 - P: 0.8352, R: 0.8662, F1: 0.8504\n",
            "  Output4 - P: 0.8334, R: 0.8587, F1: 0.8459\n",
            "  Output5 - P: 0.8142, R: 0.8636, F1: 0.8382\n",
            "  Output6 - P: 0.8785, R: 0.8666, F1: 0.8725\n",
            "  Output7 - P: 0.7771, R: 0.8477, F1: 0.8109\n",
            "  Output8 - P: 0.8292, R: 0.8583, F1: 0.8435\n",
            "平均值 - P: 0.8206, R: 0.8576, F1: 0.8381\n",
            "--------------------------------------------------\n",
            "正在处理第 27/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6250, R: 0.8299, F1: 0.7131\n",
            "  Output2 - P: 0.7871, R: 0.8222, F1: 0.8043\n",
            "  Output3 - P: 0.7318, R: 0.8217, F1: 0.7741\n",
            "  Output4 - P: 0.7163, R: 0.8284, F1: 0.7682\n",
            "  Output5 - P: 0.7422, R: 0.8289, F1: 0.7832\n",
            "  Output6 - P: 0.8254, R: 0.8592, F1: 0.8420\n",
            "  Output7 - P: 0.6408, R: 0.8077, F1: 0.7146\n",
            "  Output8 - P: 0.7627, R: 0.8292, F1: 0.7946\n",
            "平均值 - P: 0.7289, R: 0.8284, F1: 0.7743\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7609, R: 0.8906, F1: 0.8207\n",
            "  Output2 - P: 0.8559, R: 0.8853, F1: 0.8703\n",
            "  Output3 - P: 0.8261, R: 0.8871, F1: 0.8555\n",
            "  Output4 - P: 0.8176, R: 0.8873, F1: 0.8510\n",
            "  Output5 - P: 0.8397, R: 0.8908, F1: 0.8645\n",
            "  Output6 - P: 0.8919, R: 0.9093, F1: 0.9005\n",
            "  Output7 - P: 0.7916, R: 0.8784, F1: 0.8327\n",
            "  Output8 - P: 0.8530, R: 0.8805, F1: 0.8665\n",
            "平均值 - P: 0.8296, R: 0.8886, F1: 0.8577\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7461, R: 0.8678, F1: 0.8023\n",
            "  Output2 - P: 0.8595, R: 0.8579, F1: 0.8587\n",
            "  Output3 - P: 0.8174, R: 0.8618, F1: 0.8391\n",
            "  Output4 - P: 0.8076, R: 0.8678, F1: 0.8366\n",
            "  Output5 - P: 0.8329, R: 0.8668, F1: 0.8495\n",
            "  Output6 - P: 0.8690, R: 0.8935, F1: 0.8811\n",
            "  Output7 - P: 0.7689, R: 0.8622, F1: 0.8129\n",
            "  Output8 - P: 0.8468, R: 0.8586, F1: 0.8526\n",
            "平均值 - P: 0.8185, R: 0.8670, F1: 0.8416\n",
            "--------------------------------------------------\n",
            "正在处理第 28/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6004, R: 0.8242, F1: 0.6947\n",
            "  Output2 - P: 0.7798, R: 0.8270, F1: 0.8027\n",
            "  Output3 - P: 0.7431, R: 0.8286, F1: 0.7835\n",
            "  Output4 - P: 0.7292, R: 0.8144, F1: 0.7694\n",
            "  Output5 - P: 0.6890, R: 0.8182, F1: 0.7481\n",
            "  Output6 - P: 0.7707, R: 0.8457, F1: 0.8065\n",
            "  Output7 - P: 0.6516, R: 0.8187, F1: 0.7257\n",
            "  Output8 - P: 0.6715, R: 0.8116, F1: 0.7350\n",
            "平均值 - P: 0.7044, R: 0.8236, F1: 0.7582\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7635, R: 0.8870, F1: 0.8206\n",
            "  Output2 - P: 0.8717, R: 0.8871, F1: 0.8794\n",
            "  Output3 - P: 0.8493, R: 0.8901, F1: 0.8692\n",
            "  Output4 - P: 0.8474, R: 0.8889, F1: 0.8677\n",
            "  Output5 - P: 0.8198, R: 0.8821, F1: 0.8498\n",
            "  Output6 - P: 0.8640, R: 0.9033, F1: 0.8832\n",
            "  Output7 - P: 0.8041, R: 0.8823, F1: 0.8414\n",
            "  Output8 - P: 0.8183, R: 0.8900, F1: 0.8527\n",
            "平均值 - P: 0.8298, R: 0.8889, F1: 0.8580\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7275, R: 0.8762, F1: 0.7949\n",
            "  Output2 - P: 0.8590, R: 0.8674, F1: 0.8631\n",
            "  Output3 - P: 0.8319, R: 0.8748, F1: 0.8528\n",
            "  Output4 - P: 0.8304, R: 0.8767, F1: 0.8529\n",
            "  Output5 - P: 0.8023, R: 0.8710, F1: 0.8352\n",
            "  Output6 - P: 0.8429, R: 0.9035, F1: 0.8721\n",
            "  Output7 - P: 0.7759, R: 0.8723, F1: 0.8213\n",
            "  Output8 - P: 0.7964, R: 0.8616, F1: 0.8277\n",
            "平均值 - P: 0.8083, R: 0.8754, F1: 0.8400\n",
            "--------------------------------------------------\n",
            "正在处理第 29/29 行...\n",
            "\n",
            "使用模型 distilbert-base-uncased 计算BERTScore:\n",
            "  Output1 - P: 0.6460, R: 0.7790, F1: 0.7063\n",
            "  Output2 - P: 0.7863, R: 0.7872, F1: 0.7868\n",
            "  Output3 - P: 0.7316, R: 0.7940, F1: 0.7615\n",
            "  Output4 - P: 0.7395, R: 0.7798, F1: 0.7592\n",
            "  Output5 - P: 0.7192, R: 0.7828, F1: 0.7496\n",
            "  Output6 - P: 0.8031, R: 0.8001, F1: 0.8016\n",
            "  Output7 - P: 0.6468, R: 0.8014, F1: 0.7158\n",
            "  Output8 - P: 0.7051, R: 0.7910, F1: 0.7456\n",
            "平均值 - P: 0.7222, R: 0.7894, F1: 0.7533\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 xlm-roberta-large 计算BERTScore:\n",
            "  Output1 - P: 0.7698, R: 0.8756, F1: 0.8193\n",
            "  Output2 - P: 0.8690, R: 0.8723, F1: 0.8706\n",
            "  Output3 - P: 0.8305, R: 0.8712, F1: 0.8504\n",
            "  Output4 - P: 0.8450, R: 0.8705, F1: 0.8575\n",
            "  Output5 - P: 0.8297, R: 0.8745, F1: 0.8515\n",
            "  Output6 - P: 0.8703, R: 0.8840, F1: 0.8771\n",
            "  Output7 - P: 0.8034, R: 0.8732, F1: 0.8369\n",
            "  Output8 - P: 0.8242, R: 0.8678, F1: 0.8454\n",
            "平均值 - P: 0.8302, R: 0.8736, F1: 0.8511\n",
            "--------------------------------------------------\n",
            "\n",
            "使用模型 roberta-large 计算BERTScore:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output1 - P: 0.7384, R: 0.8615, F1: 0.7952\n",
            "  Output2 - P: 0.8580, R: 0.8671, F1: 0.8625\n",
            "  Output3 - P: 0.8205, R: 0.8653, F1: 0.8423\n",
            "  Output4 - P: 0.8363, R: 0.8615, F1: 0.8487\n",
            "  Output5 - P: 0.8146, R: 0.8676, F1: 0.8403\n",
            "  Output6 - P: 0.8642, R: 0.8806, F1: 0.8723\n",
            "  Output7 - P: 0.7764, R: 0.8614, F1: 0.8167\n",
            "  Output8 - P: 0.8143, R: 0.8560, F1: 0.8346\n",
            "平均值 - P: 0.8153, R: 0.8651, F1: 0.8391\n",
            "--------------------------------------------------\n",
            "处理完成，结果已保存至 D:\\liulanqi1\\MindMap-main\\MindMap-main\\实验结果\\output_bert_score_multi_models_fixed.csv\n"
          ]
        }
      ],
      "source": [
        "from evaluate import load\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 文件路径\n",
        "input_file = r'D:\\liulanqi1\\MindMap-main\\MindMap-main\\实验结果\\output.csv'\n",
        "output_file = r'D:\\liulanqi1\\MindMap-main\\MindMap-main\\实验结果\\output_bert_score_multi_models_fixed.csv'\n",
        "\n",
        "# 加载BERTScore评估器\n",
        "bertscore = load(\"bertscore\")\n",
        "\n",
        "# 定义要使用的不同模型\n",
        "models = [\n",
        "    \"distilbert-base-uncased\",  # 原来使用的模型,       \n",
        "    \"xlm-roberta-large\",             \n",
        "    \"roberta-large\",          # 如果计算资源允许，可以取消注释使用这个更大的模型\n",
        "]\n",
        "\n",
        "def clean_text_for_csv(text):\n",
        "    \"\"\"清理文本，去除可能导致CSV解析问题的字符\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return str(text)\n",
        "    # 替换换行符为空格\n",
        "    text = text.replace('\\n', ' ')\n",
        "    return text\n",
        "\n",
        "# 使用pandas读取输入CSV文件\n",
        "try:\n",
        "    df = pd.read_csv(input_file)\n",
        "    print(f\"成功读取CSV文件，共有{len(df)}行\")\n",
        "    print(f\"列名: {df.columns.tolist()}\")\n",
        "    \n",
        "    # 为每个模型和每个输出创建新列\n",
        "    for model_name in models:\n",
        "        model_short_name = model_name.split(\"-\")[0]  # 简化的模型名称\n",
        "        for i in range(1, 7):\n",
        "            df[f\"{model_short_name}_output{i}_precision\"] = np.nan\n",
        "            df[f\"{model_short_name}_output{i}_recall\"] = np.nan\n",
        "            df[f\"{model_short_name}_output{i}_f1\"] = np.nan\n",
        "        # 为每个模型添加平均值列\n",
        "        df[f\"{model_short_name}_avg_precision\"] = np.nan\n",
        "        df[f\"{model_short_name}_avg_recall\"] = np.nan\n",
        "        df[f\"{model_short_name}_avg_f1\"] = np.nan\n",
        "    \n",
        "    # 为每一行计算BERTScore\n",
        "    for idx, row in df.iterrows():\n",
        "        print(f\"正在处理第 {idx+1}/{len(df)} 行...\")\n",
        "        reference = [clean_text_for_csv(row.iloc[1])]  # 假设参考文本在第2列\n",
        "        \n",
        "        # 为每个模型计算分数\n",
        "        for model_name in models:\n",
        "            model_short_name = model_name.split(\"-\")[0]\n",
        "            print(f\"\\n使用模型 {model_name} 计算BERTScore:\")\n",
        "            \n",
        "            precision_values = []\n",
        "            recall_values = []\n",
        "            f1_values = []\n",
        "            \n",
        "            # 为每个输出计算BERTScore\n",
        "            for i in range(1, 9):\n",
        "                output_col_idx = i + 1  # 假设输出从第3列开始\n",
        "                if output_col_idx < len(row):\n",
        "                    output_text = [clean_text_for_csv(row.iloc[output_col_idx])]\n",
        "                    \n",
        "                    try:\n",
        "                        results = bertscore.compute(\n",
        "                            predictions=output_text, \n",
        "                            references=reference, \n",
        "                            model_type=model_name\n",
        "                        )\n",
        "                        \n",
        "                        precision = results[\"precision\"][0]\n",
        "                        recall = results[\"recall\"][0]\n",
        "                        f1 = results[\"f1\"][0]\n",
        "                        \n",
        "                        # 存储结果\n",
        "                        df.at[idx, f\"{model_short_name}_output{i}_precision\"] = precision\n",
        "                        df.at[idx, f\"{model_short_name}_output{i}_recall\"] = recall\n",
        "                        df.at[idx, f\"{model_short_name}_output{i}_f1\"] = f1\n",
        "                        \n",
        "                        precision_values.append(precision)\n",
        "                        recall_values.append(recall)\n",
        "                        f1_values.append(f1)\n",
        "                        \n",
        "                        print(f\"  Output{i} - P: {precision:.4f}, R: {recall:.4f}, F1: {f1:.4f}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"  计算Output{i}的BERTScore时出错: {e}\")\n",
        "                else:\n",
        "                    print(f\"  警告: 没有找到Output{i}的列\")\n",
        "            \n",
        "            # 计算平均值\n",
        "            if precision_values:\n",
        "                avg_precision = np.mean(precision_values)\n",
        "                avg_recall = np.mean(recall_values)\n",
        "                avg_f1 = np.mean(f1_values)\n",
        "                \n",
        "                df.at[idx, f\"{model_short_name}_avg_precision\"] = avg_precision\n",
        "                df.at[idx, f\"{model_short_name}_avg_recall\"] = avg_recall\n",
        "                df.at[idx, f\"{model_short_name}_avg_f1\"] = avg_f1\n",
        "                \n",
        "                print(f\"平均值 - P: {avg_precision:.4f}, R: {avg_recall:.4f}, F1: {avg_f1:.4f}\")\n",
        "            else:\n",
        "                print(\"无法计算平均值，没有有效结果\")\n",
        "            \n",
        "            print(\"-\" * 50)\n",
        "    \n",
        "    # 保存结果到CSV文件\n",
        "    df.to_csv(output_file, index=False, encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "    print(f\"处理完成，结果已保存至 {output_file}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"处理过程中出错: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "成功读取CSV文件，共有29行\n",
            "检测到的模型: ['distilbert', 'roberta', 'xlm']\n",
            "\n",
            "聚合平均结果已保存到 D:\\liulanqi1\\MindMap-main\\MindMap-main\\实验结果\\bertscore_output_averages.csv\n",
            "\n",
            "各输出的平均BERTScore (按F1从高到低排序):\n",
            "================================================================================\n",
            "          输出名称            |   平均Precision   |    平均Recall     |      平均F1      \n",
            "--------------------------------------------------------------------------------\n",
            "KG_self-consistency (output6) |     0.8316      |     0.8577      |     0.8443     \n",
            "GPT3.5 (output2)          |     0.8263      |     0.8387      |     0.8321     \n",
            "Embedding_retrieval (output4) |     0.7930      |     0.8467      |     0.8187     \n",
            "BM25_retrieval (output3)  |     0.7900      |     0.8429      |     0.8153     \n",
            "KG_retrieval (output5)    |     0.7814      |     0.8451      |     0.8116     \n",
            "GPT4 (output8)            |     0.7814      |     0.8407      |     0.8095     \n",
            "KG_TreeOfThoughts (output7) |     0.7363      |     0.8341      |     0.7815     \n",
            "MindMap (output1)         |     0.7075      |     0.8463      |     0.7701     \n",
            "--------------------------------------------------------------------------------\n",
            "OVERALL                   |     0.7809      |     0.8440      |     0.8104     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "各模型的平均BERTScore (所有输出平均):\n",
            "=================================================================\n",
            "         模型          |   平均Precision   |    平均Recall     |      平均F1      \n",
            "-----------------------------------------------------------------\n",
            "distilbert           |     0.7109      |     0.7993      |     0.7513     \n",
            "roberta              |     0.8053      |     0.8570      |     0.8298     \n",
            "xlm                  |     0.8267      |     0.8757      |     0.8501     \n",
            "-----------------------------------------------------------------\n",
            "\n",
            "各模型在各输出上的详细平均BERTScore (按F1从高到低排序):\n",
            "====================================================================================================\n",
            "         模型          |           输出名称            |   平均Precision   |    平均Recall     |      平均F1      \n",
            "----------------------------------------------------------------------------------------------------\n",
            "xlm                  | KG_self-consistency (output6) |     0.8655      |     0.8878      |     0.8764     \n",
            "xlm                  | GPT3.5 (output2)          |     0.8621      |     0.8706      |     0.8661     \n",
            "roberta              | KG_self-consistency (output6) |     0.8439      |     0.8728      |     0.8580     \n",
            "xlm                  | Embedding_retrieval (output4) |     0.8357      |     0.8766      |     0.8556     \n",
            "xlm                  | BM25_retrieval (output3)  |     0.8333      |     0.8749      |     0.8534     \n",
            "xlm                  | KG_retrieval (output5)    |     0.8279      |     0.8771      |     0.8516     \n",
            "roberta              | GPT3.5 (output2)          |     0.8506      |     0.8520      |     0.8512     \n",
            "xlm                  | GPT4 (output8)            |     0.8269      |     0.8720      |     0.8487     \n",
            "roberta              | Embedding_retrieval (output4) |     0.8172      |     0.8592      |     0.8376     \n",
            "roberta              | BM25_retrieval (output3)  |     0.8151      |     0.8562      |     0.8351     \n",
            "roberta              | KG_retrieval (output5)    |     0.8065      |     0.8573      |     0.8310     \n",
            "xlm                  | KG_TreeOfThoughts (output7) |     0.7963      |     0.8670      |     0.8300     \n",
            "roberta              | GPT4 (output8)            |     0.8100      |     0.8494      |     0.8291     \n",
            "xlm                  | MindMap (output1)         |     0.7658      |     0.8798      |     0.8187     \n",
            "roberta              | KG_TreeOfThoughts (output7) |     0.7680      |     0.8478      |     0.8058     \n",
            "distilbert           | KG_self-consistency (output6) |     0.7855      |     0.8126      |     0.7985     \n",
            "roberta              | MindMap (output1)         |     0.7309      |     0.8614      |     0.7907     \n",
            "distilbert           | GPT3.5 (output2)          |     0.7661      |     0.7933      |     0.7791     \n",
            "distilbert           | Embedding_retrieval (output4) |     0.7259      |     0.8043      |     0.7628     \n",
            "distilbert           | BM25_retrieval (output3)  |     0.7217      |     0.7974      |     0.7573     \n",
            "distilbert           | KG_retrieval (output5)    |     0.7097      |     0.8010      |     0.7522     \n",
            "distilbert           | GPT4 (output8)            |     0.7074      |     0.8007      |     0.7508     \n",
            "distilbert           | KG_TreeOfThoughts (output7) |     0.6447      |     0.7876      |     0.7087     \n",
            "distilbert           | MindMap (output1)         |     0.6257      |     0.7979      |     0.7008     \n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "input_file = r'D:\\liulanqi1\\MindMap-main\\MindMap-main\\实验结果\\output_bert_score_multi_models_fixed.csv'\n",
        "output_file = r'D:\\liulanqi1\\MindMap-main\\MindMap-main\\实验结果\\bertscore_output_averages.csv'\n",
        "\n",
        "# 定义 output 对应的真实名称\n",
        "output_name_map = {\n",
        "    'output1': 'MindMap',\n",
        "    'output2': 'GPT3.5',\n",
        "    'output3': 'BM25_retrieval',\n",
        "    'output4': 'Embedding_retrieval',\n",
        "    'output5': 'KG_retrieval',\n",
        "    'output6': 'KG_self-consistency',\n",
        "    'output7': 'KG_TreeOfThoughts',\n",
        "    'output8': 'GPT4'\n",
        "}\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(input_file)\n",
        "    print(f\"成功读取CSV文件，共有{len(df)}行\")\n",
        "\n",
        "    # Detect model prefixes from column names\n",
        "    model_prefixes = set()\n",
        "    for col in df.columns:\n",
        "        if '_output' in col and ('_precision' in col or '_recall' in col or '_f1' in col):\n",
        "            prefix = col.split('_output', 1)[0]\n",
        "            model_prefixes.add(prefix)\n",
        "\n",
        "    print(f\"检测到的模型: {sorted(list(model_prefixes))}\")\n",
        "\n",
        "    # 1. Calculate average score ACROSS MODELS for each output\n",
        "    results = []\n",
        "    for i in range(1, 9):\n",
        "        output_metrics = {'output_number': f'output{i}', 'avg_precision': 0.0, 'avg_recall': 0.0, 'avg_f1': 0.0, 'count': 0}\n",
        "        for prefix in model_prefixes:\n",
        "            precision_col = f\"{prefix}_output{i}_precision\"\n",
        "            recall_col = f\"{prefix}_output{i}_recall\"\n",
        "            f1_col = f\"{prefix}_output{i}_f1\"\n",
        "\n",
        "            if all(col in df.columns for col in [precision_col, recall_col, f1_col]):\n",
        "                avg_precision = df[precision_col].mean(skipna=True)\n",
        "                avg_recall = df[recall_col].mean(skipna=True)\n",
        "                avg_f1 = df[f1_col].mean(skipna=True)\n",
        "\n",
        "                if not np.isnan(avg_precision) and not np.isnan(avg_recall) and not np.isnan(avg_f1):\n",
        "                    output_metrics['avg_precision'] += avg_precision\n",
        "                    output_metrics['avg_recall'] += avg_recall\n",
        "                    output_metrics['avg_f1'] += avg_f1\n",
        "                    output_metrics['count'] += 1\n",
        "\n",
        "        if output_metrics['count'] > 0:\n",
        "            for metric in ['avg_precision', 'avg_recall', 'avg_f1']:\n",
        "                output_metrics[metric] /= output_metrics['count']\n",
        "            results.append(output_metrics)\n",
        "\n",
        "    # 2. Calculate OVERALL average across all models and outputs\n",
        "    if results:\n",
        "        overall_avg = {\n",
        "            'output_number': 'overall',\n",
        "            'avg_precision': np.mean([r['avg_precision'] for r in results]),\n",
        "            'avg_recall': np.mean([r['avg_recall'] for r in results]),\n",
        "            'avg_f1': np.mean([r['avg_f1'] for r in results])\n",
        "        }\n",
        "        results.append(overall_avg)\n",
        "\n",
        "    # 3. Per-model averages + detailed per-output per-model\n",
        "    model_results = []\n",
        "    detailed_model_output_results = []\n",
        "\n",
        "    for prefix in sorted(list(model_prefixes)):\n",
        "        model_metrics = {'model': prefix, 'avg_precision': 0.0, 'avg_recall': 0.0, 'avg_f1': 0.0, 'output_count': 0}\n",
        "        for i in range(1, 9):\n",
        "            precision_col = f\"{prefix}_output{i}_precision\"\n",
        "            recall_col = f\"{prefix}_output{i}_recall\"\n",
        "            f1_col = f\"{prefix}_output{i}_f1\"\n",
        "\n",
        "            if all(col in df.columns for col in [precision_col, recall_col, f1_col]):\n",
        "                avg_precision = df[precision_col].mean(skipna=True)\n",
        "                avg_recall = df[recall_col].mean(skipna=True)\n",
        "                avg_f1 = df[f1_col].mean(skipna=True)\n",
        "\n",
        "                if not np.isnan(avg_precision) and not np.isnan(avg_recall) and not np.isnan(avg_f1):\n",
        "                    detailed_model_output_results.append({\n",
        "                        'model': prefix,\n",
        "                        'output_number': f'output{i}',\n",
        "                        'avg_precision': avg_precision,\n",
        "                        'avg_recall': avg_recall,\n",
        "                        'avg_f1': avg_f1\n",
        "                    })\n",
        "\n",
        "                    model_metrics['avg_precision'] += avg_precision\n",
        "                    model_metrics['avg_recall'] += avg_recall\n",
        "                    model_metrics['avg_f1'] += avg_f1\n",
        "                    model_metrics['output_count'] += 1\n",
        "\n",
        "        if model_metrics['output_count'] > 0:\n",
        "            for metric in ['avg_precision', 'avg_recall', 'avg_f1']:\n",
        "                model_metrics[metric] /= model_metrics['output_count']\n",
        "            del model_metrics['output_count']\n",
        "            model_results.append(model_metrics)\n",
        "\n",
        "    # Save combined results\n",
        "    final_df = pd.concat([pd.DataFrame(results), pd.DataFrame(model_results)], ignore_index=True)\n",
        "    final_df.to_csv(output_file, index=False, float_format='%.6f')\n",
        "    print(f\"\\n聚合平均结果已保存到 {output_file}\")\n",
        "\n",
        "    # --- Printing Tables ---\n",
        "\n",
        "    # Table 1: Sorted by avg_f1\n",
        "    print(\"\\n各输出的平均BERTScore (按F1从高到低排序):\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"{'输出名称':^25} | {'平均Precision':^15} | {'平均Recall':^15} | {'平均F1':^15}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    sorted_results = sorted(\n",
        "        [r for r in results if r['output_number'] != 'overall'],\n",
        "        key=lambda x: x['avg_f1'],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    for r in sorted_results:\n",
        "        output_number = r.get('output_number', 'N/A')\n",
        "        display_name = output_name_map.get(output_number, output_number) + f\" ({output_number})\"\n",
        "        print(f\"{display_name:<25} | {r.get('avg_precision', 0):^15.4f} | {r.get('avg_recall', 0):^15.4f} | {r.get('avg_f1', 0):^15.4f}\")\n",
        "\n",
        "    # Print overall row\n",
        "    overall_row = next((r for r in results if r['output_number'] == 'overall'), None)\n",
        "    if overall_row:\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'OVERALL':<25} | {overall_row.get('avg_precision', 0):^15.4f} | {overall_row.get('avg_recall', 0):^15.4f} | {overall_row.get('avg_f1', 0):^15.4f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Table 2: Average per model\n",
        "    print(\"\\n各模型的平均BERTScore (所有输出平均):\")\n",
        "    print(\"=\" * 65)\n",
        "    print(f\"{'模型':^20} | {'平均Precision':^15} | {'平均Recall':^15} | {'平均F1':^15}\")\n",
        "    print(\"-\" * 65)\n",
        "    model_results_print_df = pd.DataFrame(model_results).fillna(0)\n",
        "    for index, m in model_results_print_df.iterrows():\n",
        "        print(f\"{m.get('model', 'N/A'):<20} | {m.get('avg_precision', 0):^15.4f} | {m.get('avg_recall', 0):^15.4f} | {m.get('avg_f1', 0):^15.4f}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    # Table 3: Detailed per model and output, sorted by F1\n",
        "    print(\"\\n各模型在各输出上的详细平均BERTScore (按F1从高到低排序):\")\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"{'模型':^20} | {'输出名称':^25} | {'平均Precision':^15} | {'平均Recall':^15} | {'平均F1':^15}\")\n",
        "    print(\"-\" * 100)\n",
        "    detailed_df = pd.DataFrame(detailed_model_output_results)\n",
        "    if not detailed_df.empty:\n",
        "        detailed_df = detailed_df.fillna(0)\n",
        "        # 按 avg_f1 从高到低排序\n",
        "        detailed_df = detailed_df.sort_values(by=['avg_f1'], ascending=False)\n",
        "        for _, d in detailed_df.iterrows():\n",
        "            output_number = d.get('output_number', 'N/A')\n",
        "            display_name = output_name_map.get(output_number, output_number) + f\" ({output_number})\"\n",
        "            print(f\"{d.get('model', 'N/A'):<20} | {display_name:<25} | {d.get('avg_precision', 0):^15.4f} | {d.get('avg_recall', 0):^15.4f} | {d.get('avg_f1', 0):^15.4f}\")\n",
        "        print(\"-\" * 100)\n",
        "    else:\n",
        "        print(\"⚠ 没有可用的详细模型输出数据，跳过详细表格打印。\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"错误: 输入文件 '{input_file}' 未找到。\")\n",
        "except Exception as e:\n",
        "    print(f\"处理过程中发生错误: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "成功读取CSV文件，共有29行\n",
            "检测到的模型: ['distilbert', 'roberta', 'xlm']\n",
            "已保存按F1排序的输出到 D:\\liulanqi1\\MindMap-main\\MindMap-main\\实验结果\\outputs_sorted.csv\n",
            "已保存模型平均值到 D:\\liulanqi1\\MindMap-main\\MindMap-main\\实验结果\\models_average.csv\n",
            "已保存按F1排序的详细表到 D:\\liulanqi1\\MindMap-main\\MindMap-main\\实验结果\\detailed_sorted.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "input_file = r'D:\\liulanqi1\\MindMap-main\\MindMap-main\\实验结果\\output_bert_score_multi_models_fixed.csv'\n",
        "output_file_base = r'D:\\liulanqi1\\MindMap-main\\MindMap-main\\实验结果'\n",
        "\n",
        "# 定义 output 对应的真实名称\n",
        "output_name_map = {\n",
        "    'output1': 'MindMap',\n",
        "    'output2': 'GPT3.5',\n",
        "    'output3': 'BM25_retrieval',\n",
        "    'output4': 'Embedding_retrieval',\n",
        "    'output5': 'KG_retrieval',\n",
        "    'output6': 'KG_self-consistency',\n",
        "    'output7': 'KG_TreeOfThoughts',\n",
        "    'output8': 'GPT4'\n",
        "}\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(input_file)\n",
        "    print(f\"成功读取CSV文件，共有{len(df)}行\")\n",
        "\n",
        "    model_prefixes = set()\n",
        "    for col in df.columns:\n",
        "        if '_output' in col and ('_precision' in col or '_recall' in col or '_f1' in col):\n",
        "            prefix = col.split('_output', 1)[0]\n",
        "            model_prefixes.add(prefix)\n",
        "\n",
        "    print(f\"检测到的模型: {sorted(list(model_prefixes))}\")\n",
        "\n",
        "    # 1. Calculate average score ACROSS MODELS for each output\n",
        "    results = []\n",
        "    for i in range(1, 9):\n",
        "        output_metrics = {'output_number': f'output{i}', 'output_name': output_name_map.get(f'output{i}', f'output{i}'),\n",
        "                          'avg_precision': 0.0, 'avg_recall': 0.0, 'avg_f1': 0.0, 'count': 0}\n",
        "        for prefix in model_prefixes:\n",
        "            precision_col = f\"{prefix}_output{i}_precision\"\n",
        "            recall_col = f\"{prefix}_output{i}_recall\"\n",
        "            f1_col = f\"{prefix}_output{i}_f1\"\n",
        "\n",
        "            if all(col in df.columns for col in [precision_col, recall_col, f1_col]):\n",
        "                avg_precision = df[precision_col].mean(skipna=True)\n",
        "                avg_recall = df[recall_col].mean(skipna=True)\n",
        "                avg_f1 = df[f1_col].mean(skipna=True)\n",
        "\n",
        "                if not np.isnan(avg_precision) and not np.isnan(avg_recall) and not np.isnan(avg_f1):\n",
        "                    output_metrics['avg_precision'] += avg_precision\n",
        "                    output_metrics['avg_recall'] += avg_recall\n",
        "                    output_metrics['avg_f1'] += avg_f1\n",
        "                    output_metrics['count'] += 1\n",
        "\n",
        "        if output_metrics['count'] > 0:\n",
        "            for metric in ['avg_precision', 'avg_recall', 'avg_f1']:\n",
        "                output_metrics[metric] /= output_metrics['count']\n",
        "            results.append(output_metrics)\n",
        "\n",
        "    # 2. Calculate OVERALL average\n",
        "    if results:\n",
        "        overall_avg = {\n",
        "            'output_number': 'overall',\n",
        "            'output_name': 'Overall',\n",
        "            'avg_precision': np.mean([r['avg_precision'] for r in results]),\n",
        "            'avg_recall': np.mean([r['avg_recall'] for r in results]),\n",
        "            'avg_f1': np.mean([r['avg_f1'] for r in results])\n",
        "        }\n",
        "        results.append(overall_avg)\n",
        "\n",
        "    # 3. Per-model averages + detailed per-output per-model\n",
        "    model_results = []\n",
        "    detailed_model_output_results = []\n",
        "\n",
        "    for prefix in sorted(list(model_prefixes)):\n",
        "        model_metrics = {'model': prefix, 'avg_precision': 0.0, 'avg_recall': 0.0, 'avg_f1': 0.0, 'output_count': 0}\n",
        "        for i in range(1, 9):\n",
        "            precision_col = f\"{prefix}_output{i}_precision\"\n",
        "            recall_col = f\"{prefix}_output{i}_recall\"\n",
        "            f1_col = f\"{prefix}_output{i}_f1\"\n",
        "\n",
        "            if all(col in df.columns for col in [precision_col, recall_col, f1_col]):\n",
        "                avg_precision = df[precision_col].mean(skipna=True)\n",
        "                avg_recall = df[recall_col].mean(skipna=True)\n",
        "                avg_f1 = df[f1_col].mean(skipna=True)\n",
        "\n",
        "                if not np.isnan(avg_precision) and not np.isnan(avg_recall) and not np.isnan(avg_f1):\n",
        "                    detailed_model_output_results.append({\n",
        "                        'model': prefix,\n",
        "                        'output_number': f'output{i}',\n",
        "                        'output_name': output_name_map.get(f'output{i}', f'output{i}'),\n",
        "                        'avg_precision': avg_precision,\n",
        "                        'avg_recall': avg_recall,\n",
        "                        'avg_f1': avg_f1\n",
        "                    })\n",
        "\n",
        "                    model_metrics['avg_precision'] += avg_precision\n",
        "                    model_metrics['avg_recall'] += avg_recall\n",
        "                    model_metrics['avg_f1'] += avg_f1\n",
        "                    model_metrics['output_count'] += 1\n",
        "\n",
        "        if model_metrics['output_count'] > 0:\n",
        "            for metric in ['avg_precision', 'avg_recall', 'avg_f1']:\n",
        "                model_metrics[metric] /= model_metrics['output_count']\n",
        "            del model_metrics['output_count']\n",
        "            model_results.append(model_metrics)\n",
        "\n",
        "    # === Save sorted outputs (by F1) ===\n",
        "    outputs_df = pd.DataFrame([r for r in results if r['output_number'] != 'overall'])\n",
        "    outputs_df_sorted = outputs_df.sort_values(by='avg_f1', ascending=False)\n",
        "    outputs_df_sorted.to_csv(f\"{output_file_base}\\\\outputs_sorted.csv\", index=False, float_format='%.6f')\n",
        "    print(f\"已保存按F1排序的输出到 {output_file_base}\\\\outputs_sorted.csv\")\n",
        "\n",
        "    # === Save overall model averages ===\n",
        "    model_df = pd.DataFrame(model_results)\n",
        "    model_df.to_csv(f\"{output_file_base}\\\\models_average.csv\", index=False, float_format='%.6f')\n",
        "    print(f\"已保存模型平均值到 {output_file_base}\\\\models_average.csv\")\n",
        "\n",
        "    # === Save detailed per-model per-output sorted by F1 ===\n",
        "    detailed_df = pd.DataFrame(detailed_model_output_results)\n",
        "    detailed_df_sorted = detailed_df.sort_values(by='avg_f1', ascending=False)\n",
        "    detailed_df_sorted.to_csv(f\"{output_file_base}\\\\detailed_sorted.csv\", index=False, float_format='%.6f')\n",
        "    print(f\"已保存按F1排序的详细表到 {output_file_base}\\\\detailed_sorted.csv\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"错误: 输入文件 '{input_file}' 未找到。\")\n",
        "except Exception as e:\n",
        "    print(f\"处理过程中发生错误: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
